# GestureSynth â€“ Hand-Gesture MIDI Controller


GestureSynth is a real-time, vision-based MIDI controller that translates hand movement into musical notes. Using MediaPipe for hand tracking and PyQt5 for the interface, it maps hand position to pitch, velocity, and even chordsâ€”routing output to any DAW or virtual instrument.

---

## âœ¨ Features

- **Real-time gesture-to-MIDI conversion** at 30 FPS
- **Multiple velocity modes**: vertical, horizontal, or fixed
- **Chord mode**: play major, minor, or 7th chords with one hand
- **Automatic parameter tuning** via LLM analysis of your playing style
- **MIDI recording & export** as standard `.mid` files
- **Fully offline processing** â€“ your webcam data never leaves your machine
- **Responsive, HCI-optimized UI** following Fittsâ€™s Law, Hickâ€™s Law, and Gestalt principles

---

## ğŸ–¥ï¸ Requirements

- Python 3.8+
- A standard webcam
- MIDI-compatible software or hardware (e.g., Ableton Live, FL Studio, GarageBand, Microsoft GS Wavetable Synth)

---

## ğŸš€ Installation

### 1. Clone the repository
```bash
git clone https://github.com/your-username/gesturesynth.git
cd gesturesynth
```

---
## ğŸ“ Project Structure

<img width="783" height="233" alt="image" src="https://github.com/user-attachments/assets/5338e346-e6f4-426a-b89c-371d671aae7c" />


---
## ğŸ¯ Use Cases

    Music Education: Visual pitch/velocity feedback for students
    Live Performance: Expressive, wireless gestural control
    Accessibility: Play music without physical instruments
    Prototyping: Test musical ideas through movement
---
## Acknowledgements

    MediaPipe by Google for real-time hand tracking
    Mido for MIDI handling in Python
    Groq for fast LLM inference
    PyQt5 for the cross-platform desktop interface
